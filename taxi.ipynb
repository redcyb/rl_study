{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "Observation space: Discrete(500)\n",
      "Initial state: 309\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "s = env.reset()\n",
    "print(f\"Initial state: {s}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       ...,\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.ones((n_states, n_actions), dtype=\"float\") / n_actions\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(policy, t_max=int(10**4)):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = np.random.choice(n_actions, p=policy[s])\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # Record state, action and add up reward to states,actions and total_reward accordingly.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r = generate_session(policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float, np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9afdce1f28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see the initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "    \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "#     elite_states = [s for idx, s in enumerate(states_batch) if rewards_batch[idx] >= reward_threshold]\n",
    "#     elite_actions = [a for idx, a in enumerate(actions_batch) if rewards_batch[idx] >= reward_threshold]\n",
    "\n",
    "    elite_mask = np.asarray(rewards_batch) >= reward_threshold\n",
    "    elite_states = np.asarray(states_batch)[elite_mask]\n",
    "    elite_actions = np.asarray(actions_batch)[elite_mask]\n",
    "\n",
    "    return np.concatenate(elite_states), np.concatenate(elite_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "\n",
    "test_result_40 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "\n",
    "test_result_90 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "\n",
    "test_result_100 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "    np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "    np.all(test_result_90[1] == [3, 3]),\\\n",
    "    \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and\\\n",
    "    np.all(test_result_100[1] == [3, 3]),\\\n",
    "    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "    \n",
    "    for idx, s in enumerate(elite_states):\n",
    "        new_policy[s, elite_actions[idx]] += 1.\n",
    "        \n",
    "    never_visited_mask = new_policy.sum(axis=1) == 0.\n",
    "    new_policy[never_visited_mask] = 1.\n",
    "\n",
    "    new_policy /= new_policy.sum(axis=1)[:, None]\n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\n",
    "elite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(\n",
    "), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "\n",
    "assert np.all(\n",
    "    new_policy >= 0\n",
    "), \"Your new policy can't have negative action probabilities\"\n",
    "\n",
    "assert np.allclose(\n",
    "    new_policy.sum(\n",
    "    axis=-1), 1\n",
    "), \"Your new policy should be a valid probability distribution over actions\"\n",
    "\n",
    "reference_answer = np.array([\n",
    "    [1.,  0.,  0.,  0.,  0.],\n",
    "    [0.5,  0.,  0.,  0.5,  0.],\n",
    "    [0.,  0.33333333,  0.66666667,  0.,  0.],\n",
    "    [0.,  0.,  0.,  0.5,  0.5]])\n",
    "\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    display.clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones([n_states, n_actions]) / n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 250  # sample this many sessions\n",
    "percentile = 30  # take this percent of session with highest rewards\n",
    "learning_rate = 0.5  # add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "    #  [ < generate a list of n_sessions new sessions > ]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "    \n",
    "#     mean_reward = np.mean(rewards_batch)\n",
    "#     log.append(mean_reward)\n",
    "    \n",
    "    if i+1 % 5:\n",
    "        learning_rate *= 0.99\n",
    "    \n",
    "    elite_states, elite_actions = select_elites(\n",
    "        states_batch, \n",
    "        actions_batch, \n",
    "        rewards_batch, \n",
    "        percentile=percentile\n",
    "    )\n",
    "\n",
    "    new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "    policy = learning_rate * new_policy + (1-learning_rate) * policy\n",
    "\n",
    "    # display results on chart\n",
    "    show_progress(rewards_batch, log, percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "s = env.reset()\n",
    "\n",
    "while True:\n",
    "    a = np.random.choice(n_actions, p=policy[s])\n",
    "    s, r, done, info = env.step(a)\n",
    "    \n",
    "    display.clear_output()\n",
    "    env.render()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
